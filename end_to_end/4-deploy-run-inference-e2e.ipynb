{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Insurance Fraud Detection, Part 4 : Deploy, Run Inference, Interpret Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the end to end use case, we will deploy the mitigated model that is the end-product of this fraud detection use-case. We will show how to run inference and also how to use Clarify to interpret or \"explain\" the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overview-4'></a>\n",
    "\n",
    "## [Overview](./0-AutoClaimFraudDetection.ipynb)\n",
    "* ### [Notebook 1](./1-data-prep-e2e.ipynb)\n",
    "  * #### [Getting started](#aud-getting-started)\n",
    "  * #### [DataSets](#aud-datasets)\n",
    "  * #### [SageMaker Feature Store](#aud-feature-store)\n",
    "  * #### [Create train and test datasets](#aud-dataset)\n",
    "* ### [Notebook 2](./2-lineage-train-assess-bias-tune-registry-e2e.ipynb)\n",
    "  * #### [Train a model using XGBoost](#aud-train-model)\n",
    "  * #### [Model lineage with artifacts and associations](#model-lineage)\n",
    "  * #### [Evaluate the model for bias with Clarify](#check-bias)\n",
    "  * #### [Deposit Model and Lineage in SageMaker Model Registry](#model-registry)\n",
    "* ### [Notebook 3](./3-mitigate-bias-train-model2-registry-e2e.ipynb)\n",
    "  * #### [Develop a second model](#second-model)\n",
    "  * #### [Analyze the Second Model for Bias](#analyze-second-model)\n",
    "  * #### [View Results of Clarify Bias Detection Job](#view-second-clarify-job)\n",
    "  * #### [Configure and Run Clarify Explainability Job](#explainability)\n",
    "  * #### [Create Model Package for second trained model](#model-package)\n",
    "  \n",
    "* ### [Notebook 4](./4-deploy-run-inference-e2e.ipynb)\n",
    "  * #### [Architecture](#deploy)\n",
    "  * #### [Deploy an approved model and Run Inference via Feature Store](#deploy-model)\n",
    "  * #### [Create a Predictor](#predictor)\n",
    "  * #### [Run  Predictions from Online FeatureStore](#run-predictions)\n",
    "* ### [Notebook 5](./5-pipeline-e2e.ipynb)\n",
    "  * #### SageMaker Pipeline\n",
    "  * #### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything you may need to create them again or it may be your first time running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket                          -> 'sagemaker-us-east-2-738335684114'\n",
      "claims_fg_name                  -> 'fraud-detect-demo-claims'\n",
      "claims_table                    -> 'fraud-detect-demo-claims-1610061189'\n",
      "col_order                       -> ['fraud', 'incident_type_theft', 'policy_state_ca'\n",
      "customers_fg_name               -> 'fraud-detect-demo-customers'\n",
      "customers_table                 -> 'fraud-detect-demo-customers-1610061192'\n",
      "database_name                   -> 'sagemaker_featurestore'\n",
      "hyperparameters                 -> {'max_depth': '3', 'eta': '0.2', 'objective': 'bin\n",
      "model_1_name                    -> 'fraud-detect-demo-xgboost-pre-smote'\n",
      "model_2_name                    -> 'fraud-detect-demo-xgboost-post-smote'\n",
      "mp2_arn                         -> 'arn:aws:sagemaker:us-east-2:738335684114:model-pa\n",
      "mpg_name                        -> 'fraud-detect-demo'\n",
      "prefix                          -> 'fraud-detect-demo'\n",
      "test_data_uri                   -> 's3://sagemaker-us-east-2-738335684114/fraud-detec\n",
      "train_data_uri                  -> 's3://sagemaker-us-east-2-738335684114/fraud-detec\n",
      "training_job_1_name             -> 'sagemaker-xgboost-2021-01-24-00-54-41-852'\n",
      "training_job_2_name             -> 'sagemaker-xgboost-2021-02-01-03-37-06-845'\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -q awswrangler==2.2.0 imbalanced-learn==0.7.0 sagemaker==2.23.1 boto3==1.16.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region, boto3 and SageMaker SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(region_name=region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables used for parameterizing the notebook run\n",
    "endpoint_name = f'{model_2_name}-endpoint'\n",
    "endpoint_instance_count = 1\n",
    "endpoint_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.c5.xlarge\"\n",
    "batch_transform_instance_count = 1\n",
    "batch_transform_instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We assume you have run the previous data prep notebook,features are in the feature store, data is imported from them into an S3 bucket specified above as BUCKET and a train and test split has been done and put in the S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='deploy'> </a>\n",
    "### Architecture for this ML Lifecycle Stage : Train, Check Bias, Tune, Record Lineage, Register Model\n",
    "\n",
    "![train-assess-tune-register](./images/e2e-3-pipeline-v3b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='deploy-model'></a>\n",
    "\n",
    "## Deploy an approved model and make prediction via Feature Store\n",
    "\n",
    "[overview](#overview-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approve second model\n",
    "In the real-life MLOps lifecycle, a model package gets approved after evaluation by data scientists, subject matter experts and auditors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model_package = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)['ModelPackageSummaryList'][0]\n",
    "model_package_update = {\n",
    "    'ModelPackageArn': second_model_package['ModelPackageArn'],\n",
    "    'ModelApprovalStatus': 'Approved'\n",
    "}\n",
    "\n",
    "update_response = sagemaker_boto_client.update_model_package(**model_package_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create endpoint config and endpoint\n",
    "Deploying the endpoint may take ~8min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_container = {'ModelPackageName': second_model_package['ModelPackageArn']}\n",
    "endpoint_config_name=f'{model_2_name}-endpoint-config'\n",
    "existing_configs = sagemaker_boto_client.list_endpoint_configs(NameContains=endpoint_config_name, MaxResults = 30)['EndpointConfigs'][0]['EndpointConfigName']\n",
    "\n",
    "if not existing_configs:\n",
    "    create_ep_config_response = sagemaker_boto_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[{\n",
    "            'InstanceType': endpoint_instance_type,\n",
    "            'InitialVariantWeight': 1,\n",
    "            'InitialInstanceCount': endpoint_instance_count,\n",
    "            'ModelName': model_2_name,\n",
    "            'VariantName': 'AllTraffic'\n",
    "        }]\n",
    "    )\n",
    "    %store endpoint_config_name\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_endpoints = sagemaker_boto_client.list_endpoints(NameContains=endpoint_name, MaxResults = 30)['Endpoints']\n",
    "if not existing_endpoints:\n",
    "    create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        EndpointConfigName=endpoint_config_name)\n",
    "    %store endpoint_name\n",
    "\n",
    "endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = endpoint_info['EndpointStatus']\n",
    "\n",
    "while endpoint_status == 'Creating':\n",
    "    endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = endpoint_info['EndpointStatus']\n",
    "    print('Endpoint status:', endpoint_status)\n",
    "    if endpoint_status == 'Creating':\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predictor'> </a>\n",
    "\n",
    "### Create a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample a claim from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')\n",
    "train = dataset.sample(frac=0.8, random_state=0)\n",
    "test = dataset.drop(train.index)\n",
    "sample_policy_id  = int(test.sample(1)['policy_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4000 entries, 10 to 19994\n",
      "Data columns (total 48 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Unnamed: 0                       4000 non-null   int64  \n",
      " 1   policy_id                        4000 non-null   int64  \n",
      " 2   incident_type_theft              4000 non-null   int64  \n",
      " 3   policy_state_ca                  4000 non-null   int64  \n",
      " 4   policy_deductable                4000 non-null   int64  \n",
      " 5   num_witnesses                    4000 non-null   int64  \n",
      " 6   policy_state_or                  4000 non-null   int64  \n",
      " 7   incident_month                   4000 non-null   int64  \n",
      " 8   customer_gender_female           4000 non-null   int64  \n",
      " 9   num_insurers_past_5_years        4000 non-null   int64  \n",
      " 10  customer_gender_male             4000 non-null   int64  \n",
      " 11  total_claim_amount               4000 non-null   float64\n",
      " 12  authorities_contacted_police     4000 non-null   int64  \n",
      " 13  incident_day                     4000 non-null   int64  \n",
      " 14  collision_type_side              4000 non-null   int64  \n",
      " 15  customer_age                     4000 non-null   int64  \n",
      " 16  customer_education               4000 non-null   int64  \n",
      " 17  driver_relationship_child        4000 non-null   int64  \n",
      " 18  driver_relationship_spouse       4000 non-null   int64  \n",
      " 19  injury_claim                     4000 non-null   float64\n",
      " 20  incident_dow                     4000 non-null   int64  \n",
      " 21  collision_type_na                4000 non-null   int64  \n",
      " 22  incident_severity                4000 non-null   int64  \n",
      " 23  driver_relationship_self         4000 non-null   int64  \n",
      " 24  num_claims_past_year             4000 non-null   int64  \n",
      " 25  months_as_customer               4000 non-null   int64  \n",
      " 26  auto_year                        4000 non-null   int64  \n",
      " 27  num_vehicles_involved            4000 non-null   int64  \n",
      " 28  policy_state_nv                  4000 non-null   int64  \n",
      " 29  authorities_contacted_ambulance  4000 non-null   int64  \n",
      " 30  num_injuries                     4000 non-null   int64  \n",
      " 31  policy_liability                 4000 non-null   int64  \n",
      " 32  police_report_available          4000 non-null   int64  \n",
      " 33  driver_relationship_na           4000 non-null   int64  \n",
      " 34  incident_type_breakin            4000 non-null   int64  \n",
      " 35  authorities_contacted_none       4000 non-null   int64  \n",
      " 36  authorities_contacted_fire       4000 non-null   int64  \n",
      " 37  driver_relationship_other        4000 non-null   int64  \n",
      " 38  policy_state_id                  4000 non-null   int64  \n",
      " 39  incident_hour                    4000 non-null   int64  \n",
      " 40  vehicle_claim                    4000 non-null   float64\n",
      " 41  fraud                            4000 non-null   int64  \n",
      " 42  incident_type_collision          4000 non-null   int64  \n",
      " 43  policy_annual_premium            4000 non-null   int64  \n",
      " 44  policy_state_az                  4000 non-null   int64  \n",
      " 45  policy_state_wa                  4000 non-null   int64  \n",
      " 46  collision_type_rear              4000 non-null   int64  \n",
      " 47  collision_type_front             4000 non-null   int64  \n",
      "dtypes: float64(3), int64(45)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sample's claim data from online feature store\n",
    "This will simulate getting data in real-time from a customer's insurance claim submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run-predictions'> </a>\n",
    "## Run Predictions on Multiple Claims\n",
    "\n",
    "[overview](#overview-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probablitity the claim from policy 4227 is fraudulent: 0.02327040024101734\n",
      "Probablitity the claim from policy 1356 is fraudulent: 0.01118443626910448\n",
      "Probablitity the claim from policy 1731 is fraudulent: 0.003912070766091347\n",
      "Probablitity the claim from policy 4944 is fraudulent: 0.045740846544504166\n",
      "Probablitity the claim from policy 1187 is fraudulent: 0.003912070766091347\n",
      "Probablitity the claim from policy 871 is fraudulent: 0.019201211631298065\n",
      "Probablitity the claim from policy 218 is fraudulent: 0.07194169610738754\n",
      "Probablitity the claim from policy 1463 is fraudulent: 0.02203090861439705\n",
      "Probablitity the claim from policy 4442 is fraudulent: 0.017646918073296547\n",
      "Probablitity the claim from policy 578 is fraudulent: 0.08237599581480026\n",
      "Probablitity the claim from policy 2182 is fraudulent: 0.008739297278225422\n",
      "Probablitity the claim from policy 4021 is fraudulent: 0.007248613052070141\n",
      "Probablitity the claim from policy 3346 is fraudulent: 0.04129285365343094\n",
      "Probablitity the claim from policy 3847 is fraudulent: 0.008639536798000336\n",
      "Probablitity the claim from policy 1134 is fraudulent: 0.02309166081249714\n",
      "Probablitity the claim from policy 4825 is fraudulent: 0.026153018698096275\n",
      "Probablitity the claim from policy 388 is fraudulent: 0.015287873335182667\n",
      "Probablitity the claim from policy 1442 is fraudulent: 0.0072795026935637\n",
      "Probablitity the claim from policy 1821 is fraudulent: 0.0077530937269330025\n",
      "Probablitity the claim from policy 4840 is fraudulent: 0.011392354033887386\n",
      "Probablitity the claim from policy 3744 is fraudulent: 0.005626254249364138\n",
      "Probablitity the claim from policy 1794 is fraudulent: 0.015348764136433601\n",
      "Probablitity the claim from policy 4301 is fraudulent: 0.053253352642059326\n",
      "Probablitity the claim from policy 538 is fraudulent: 0.004720746073871851\n",
      "Probablitity the claim from policy 4058 is fraudulent: 0.009097563102841377\n",
      "Probablitity the claim from policy 2020 is fraudulent: 0.00803267490118742\n",
      "Probablitity the claim from policy 3034 is fraudulent: 0.029522309079766273\n",
      "Probablitity the claim from policy 4591 is fraudulent: 0.01367956679314375\n",
      "Probablitity the claim from policy 2547 is fraudulent: 0.01874932460486889\n",
      "Probablitity the claim from policy 4807 is fraudulent: 0.011290277354419231\n",
      "Probablitity the claim from policy 166 is fraudulent: 0.008316769264638424\n",
      "Probablitity the claim from policy 3232 is fraudulent: 0.0078433221206069\n",
      "Probablitity the claim from policy 2859 is fraudulent: 0.004588690120726824\n",
      "Probablitity the claim from policy 3077 is fraudulent: 0.04129285365343094\n",
      "Probablitity the claim from policy 1862 is fraudulent: 0.013857879675924778\n",
      "Probablitity the claim from policy 2554 is fraudulent: 0.010174770839512348\n",
      "Probablitity the claim from policy 2394 is fraudulent: 0.003630558028817177\n",
      "Probablitity the claim from policy 1180 is fraudulent: 0.006809301674365997\n",
      "Probablitity the claim from policy 2581 is fraudulent: 0.014956960454583168\n",
      "Probablitity the claim from policy 3537 is fraudulent: 0.015287873335182667\n",
      "Probablitity the claim from policy 2379 is fraudulent: 0.013510847464203835\n",
      "Probablitity the claim from policy 1542 is fraudulent: 0.01532312948256731\n",
      "Probablitity the claim from policy 2280 is fraudulent: 0.010492981411516666\n",
      "Probablitity the claim from policy 2405 is fraudulent: 0.012727153487503529\n",
      "Probablitity the claim from policy 987 is fraudulent: 0.01549496129155159\n",
      "Probablitity the claim from policy 3445 is fraudulent: 0.004727635532617569\n",
      "Probablitity the claim from policy 3631 is fraudulent: 0.007734941318631172\n",
      "Probablitity the claim from policy 102 is fraudulent: 0.009353230707347393\n",
      "Probablitity the claim from policy 3245 is fraudulent: 0.0856315866112709\n",
      "Probablitity the claim from policy 1078 is fraudulent: 0.015552668832242489\n",
      "Probablitity the claim from policy 2926 is fraudulent: 0.09903927892446518\n",
      "Probablitity the claim from policy 1037 is fraudulent: 0.007650473620742559\n",
      "Probablitity the claim from policy 3811 is fraudulent: 0.013845029287040234\n",
      "Probablitity the claim from policy 1394 is fraudulent: 0.0053778402507305145\n",
      "Probablitity the claim from policy 3825 is fraudulent: 0.0068176789209246635\n",
      "Probablitity the claim from policy 1502 is fraudulent: 0.009276721626520157\n",
      "Probablitity the claim from policy 1089 is fraudulent: 0.007581275887787342\n",
      "Probablitity the claim from policy 4549 is fraudulent: 0.022857151925563812\n",
      "Probablitity the claim from policy 208 is fraudulent: 0.013703952543437481\n",
      "Probablitity the claim from policy 4887 is fraudulent: 0.01118443626910448\n",
      "Probablitity the claim from policy 3914 is fraudulent: 0.007650473620742559\n",
      "Probablitity the claim from policy 4313 is fraudulent: 0.018844367936253548\n",
      "Probablitity the claim from policy 2773 is fraudulent: 0.01227802224457264\n",
      "Probablitity the claim from policy 3672 is fraudulent: 0.03769160807132721\n",
      "Probablitity the claim from policy 1263 is fraudulent: 0.011326754465699196\n",
      "Probablitity the claim from policy 866 is fraudulent: 0.01235600933432579\n",
      "Probablitity the claim from policy 531 is fraudulent: 0.007449634373188019\n",
      "Probablitity the claim from policy 3741 is fraudulent: 0.03309105709195137\n",
      "Probablitity the claim from policy 1741 is fraudulent: 0.03140891343355179\n",
      "Probablitity the claim from policy 1270 is fraudulent: 0.017027119174599648\n",
      "Probablitity the claim from policy 971 is fraudulent: 0.01808340661227703\n",
      "Probablitity the claim from policy 2001 is fraudulent: 0.0073160468600690365\n",
      "Probablitity the claim from policy 995 is fraudulent: 0.03236078843474388\n",
      "Probablitity the claim from policy 1518 is fraudulent: 0.005914187990128994\n",
      "Probablitity the claim from policy 4481 is fraudulent: 0.10920623689889908\n",
      "Probablitity the claim from policy 1462 is fraudulent: 0.004588690120726824\n",
      "Probablitity the claim from policy 987 is fraudulent: 0.01549496129155159\n",
      "Probablitity the claim from policy 677 is fraudulent: 0.009353230707347393\n",
      "Probablitity the claim from policy 2912 is fraudulent: 0.0052198851481080055\n",
      "Probablitity the claim from policy 4548 is fraudulent: 0.021798400208353996\n",
      "Probablitity the claim from policy 1206 is fraudulent: 0.014156831428408623\n",
      "Probablitity the claim from policy 2786 is fraudulent: 0.010342530906200409\n",
      "Probablitity the claim from policy 2901 is fraudulent: 0.01959005743265152\n",
      "Probablitity the claim from policy 2030 is fraudulent: 0.011392795480787754\n",
      "Probablitity the claim from policy 4726 is fraudulent: 0.006167407613247633\n",
      "Probablitity the claim from policy 1836 is fraudulent: 0.01725957542657852\n",
      "Probablitity the claim from policy 4337 is fraudulent: 0.032906439155340195\n",
      "Probablitity the claim from policy 4585 is fraudulent: 0.00989462062716484\n",
      "Probablitity the claim from policy 3657 is fraudulent: 0.011852333322167397\n",
      "Probablitity the claim from policy 3939 is fraudulent: 0.011735350824892521\n",
      "Probablitity the claim from policy 3566 is fraudulent: 0.007755562663078308\n",
      "Probablitity the claim from policy 1640 is fraudulent: 0.03561621904373169\n",
      "Probablitity the claim from policy 2984 is fraudulent: 0.004588690120726824\n",
      "Probablitity the claim from policy 1919 is fraudulent: 0.03475777432322502\n",
      "Probablitity the claim from policy 4055 is fraudulent: 0.005027001723647118\n",
      "Probablitity the claim from policy 65 is fraudulent: 0.010342530906200409\n",
      "Probablitity the claim from policy 893 is fraudulent: 0.004588690120726824\n",
      "Probablitity the claim from policy 3785 is fraudulent: 0.007248613052070141\n",
      "Probablitity the claim from policy 3609 is fraudulent: 0.012194972485303879\n",
      "Probablitity the claim from policy 1244 is fraudulent: 0.009145423769950867\n"
     ]
    }
   ],
   "source": [
    "import datetime  as datetime\n",
    "timer =[]\n",
    "MAXRECS = 100\n",
    "\n",
    "def barrage_of_inference():\n",
    "    sample_policy_id  = int(test.sample(1)['policy_id'])\n",
    "    \n",
    "    temp_fg_name = 'fraud-detect-demo-claims'\n",
    "\n",
    "    claims_response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=temp_fg_name, \n",
    "        RecordIdentifierValueAsString= str(sample_policy_id)\n",
    "\n",
    "    )\n",
    "\n",
    "    if (claims_response.get('Record')):\n",
    "        claims_record = claims_response['Record']\n",
    "        claims_df = pd.DataFrame(claims_record).set_index('FeatureName')\n",
    "    else:\n",
    "        print (\"No Record returned / Record Key  \\n\")\n",
    "        \n",
    "    t0 = datetime.datetime.now()\n",
    "    \n",
    "    customers_response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=customers_fg_name, \n",
    "        RecordIdentifierValueAsString=str(sample_policy_id)\n",
    "    )\n",
    "    \n",
    "    t1 = datetime.datetime.now()\n",
    "\n",
    "    customer_record = customers_response['Record']\n",
    "    customer_df = pd.DataFrame(customer_record).set_index('FeatureName')\n",
    "    \n",
    "    \n",
    "    blended_df = pd.concat([claims_df, customer_df]).loc[col_order].drop('fraud')\n",
    "    data_input = ','.join(blended_df['ValueAsString'])\n",
    "    \n",
    "    results = predictor.predict(data_input, initial_args = {\"ContentType\": \"text/csv\"})\n",
    "    prediction = json.loads(results)\n",
    "    #print (f'Probablitity the claim from policy {int(sample_policy_id)} is fraudulent:', prediction)\n",
    "    \n",
    "    arr = t1-t0\n",
    "    minutes, seconds = divmod(arr.total_seconds(), 60)\n",
    "    \n",
    "    timer.append(seconds)\n",
    "    #print (prediction, \" done in {} \".format(seconds))\n",
    "    \n",
    "    return sample_policy_id, prediction\n",
    "\n",
    "\n",
    "for i in range(MAXRECS):\n",
    "    sample_policy_id, prediction = barrage_of_inference()\n",
    "    print (f'Probablitity the claim from policy {int(sample_policy_id)} is fraudulent:', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.077019,\n",
       " 0.018236,\n",
       " 0.010011,\n",
       " 0.008005,\n",
       " 0.007163,\n",
       " 0.006872,\n",
       " 0.00804,\n",
       " 0.007495,\n",
       " 0.008866,\n",
       " 0.007994,\n",
       " 0.0075,\n",
       " 0.007073,\n",
       " 0.008033,\n",
       " 0.007152,\n",
       " 0.007621,\n",
       " 0.007443,\n",
       " 0.006649,\n",
       " 0.007119,\n",
       " 0.007337,\n",
       " 0.007584,\n",
       " 0.00646,\n",
       " 0.006511,\n",
       " 0.007164,\n",
       " 0.006222,\n",
       " 0.006938,\n",
       " 0.00641,\n",
       " 0.007141,\n",
       " 0.00726,\n",
       " 0.007458,\n",
       " 0.007501,\n",
       " 0.007825,\n",
       " 0.007531,\n",
       " 0.008141,\n",
       " 0.007019,\n",
       " 0.007374,\n",
       " 0.00713,\n",
       " 0.006917,\n",
       " 0.007048,\n",
       " 0.007414,\n",
       " 0.006961,\n",
       " 0.007019,\n",
       " 0.007504,\n",
       " 0.006098,\n",
       " 0.00718,\n",
       " 0.0067,\n",
       " 0.006769,\n",
       " 0.006372,\n",
       " 0.006789,\n",
       " 0.006601,\n",
       " 0.007083,\n",
       " 0.081883,\n",
       " 0.008172,\n",
       " 0.008337,\n",
       " 0.008729,\n",
       " 0.01084,\n",
       " 0.007802,\n",
       " 0.007802,\n",
       " 0.008018,\n",
       " 0.007442,\n",
       " 0.007585,\n",
       " 0.007348,\n",
       " 0.007294,\n",
       " 0.007045,\n",
       " 0.00722,\n",
       " 0.007495,\n",
       " 0.007367,\n",
       " 0.00769,\n",
       " 0.007377,\n",
       " 0.008347,\n",
       " 0.00727,\n",
       " 0.007091,\n",
       " 0.007144,\n",
       " 0.007761,\n",
       " 0.007414,\n",
       " 0.00746,\n",
       " 0.007498,\n",
       " 0.007593,\n",
       " 0.009609,\n",
       " 0.008017,\n",
       " 0.008341,\n",
       " 0.008218,\n",
       " 0.008451,\n",
       " 0.007196,\n",
       " 0.007453,\n",
       " 0.007275,\n",
       " 0.007665,\n",
       " 0.008648,\n",
       " 0.023717,\n",
       " 0.007462,\n",
       " 0.007732,\n",
       " 0.008362,\n",
       " 0.007605,\n",
       " 0.007406,\n",
       " 0.008586,\n",
       " 0.008358,\n",
       " 0.00839,\n",
       " 0.0088,\n",
       " 0.00814,\n",
       " 0.007971,\n",
       " 0.007928]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above \"timer\" records the first call and then subsequent calls to the online Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p95: 0.01005245, p99: 0.07706764000000003, mean: 0.00928106 for 100 distinct feature store gets\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import numpy as np\n",
    "statistics.mean(timer)  \n",
    "\n",
    "\n",
    "arr = np.array(timer)\n",
    "print(\"p95: {}, p99: {}, mean: {} for {} distinct feature store gets\".format(np.percentile(arr,95),np.percentile(arr,99),np.mean(arr), MAXRECS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull customer data from Customers feature group\n",
    "When a customer submits an insurance claim online for instant approval, the insurance company will need to pull customer-specific data from the online feature store to add to the claim data as input for a model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_response = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=customers_fg_name, \n",
    "    RecordIdentifierValueAsString=str(sample_policy_id))\n",
    "\n",
    "customer_record = customers_response['Record']\n",
    "customer_df = pd.DataFrame(customer_record).set_index('FeatureName')\n",
    "\n",
    "\n",
    "claims_response = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=claims_fg_name, \n",
    "    RecordIdentifierValueAsString=str(sample_policy_id))\n",
    "\n",
    "claims_record = claims_response['Record']\n",
    "claims_df = pd.DataFrame(claims_record).set_index('FeatureName')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the datapoint\n",
    "The datapoint must match the exact input format as the model was trained--with all features in the correct order. In this example, the `col_order` variable was saved when you created the train and test datasets earlier in the guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_df = pd.concat([claims_df, customer_df]).loc[col_order].drop('fraud')\n",
    "data_input = ','.join(blended_df['ValueAsString'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probablitity the claim from policy 1244 is fraudulent: 0.009145423769950867\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(data_input, initial_args = {\"ContentType\": \"text/csv\"})\n",
    "prediction = json.loads(results)\n",
    "print (f'Probablitity the claim from policy {int(sample_policy_id)} is fraudulent:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-workflow-pipeline'></a>\n",
    "## In the [Next Notebook](./5-pipeline-e2e.ipynb) we will show you how to put all this together in a SageMaker Pipeline\n",
    "___\n",
    "Now that as a Data Scientist, you've manually experimented with each step in our machine learning workflow, you can take certain steps to allow for faster model creation and deployment without sacrificing transparency and tracking via model lineage. In the next section you will create a pipeline which trains a new model on SageMaker, persists the model in SageMaker and then adds the model to the registry and deploys it as a SageMaker hosted endpoint."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
